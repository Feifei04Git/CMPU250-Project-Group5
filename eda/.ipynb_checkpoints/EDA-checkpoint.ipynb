{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990bee02-0738-43fc-b735-fe32ced18f5e",
   "metadata": {},
   "source": [
    "DATA LOADING and CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37a849a2-3824-4069-afa6-8a4784792945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to Install all required packages using conda (safer on Mac & Anaconda)\n",
    "#!conda install -y -c conda-forge pyarrow fsspec huggingface_hub datasets\n",
    "#!conda install -y -c conda-forge pyarrow pandas --force-reinstall\n",
    "#!pip install numpy pandas matplotlib scikit-learn statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a64abff7-f957-4a6f-86d9-bcd00a7d94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66558400-c87b-4da2-b828-86c4f79c07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_parquet(\"hf://datasets/truthfulqa/truthful_qa/generation/validation-00000-of-00001.parquet\")\n",
    "import pandas as pd\n",
    "df2 = pd.read_parquet(\"hf://datasets/truthfulqa/truthful_qa/multiple_choice/validation-00000-of-00001.parquet\")\n",
    "\n",
    "df1.to_csv(\"truthful_qa_validation.csv\", index=False)\n",
    "df2.to_csv(\"truthful_qa_validation.csv\", index=False)\n",
    "\n",
    "df1['ai_correct'] = df1.apply(lambda row: row['best_answer'] in row['correct_answers'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90398103-f229-4c63-b494-083f2c654be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"truthfulqa/truthful_qa\", \"generation\")\n",
    "\n",
    "hedging_words = ['maybe', 'might', 'could', 'possibly', 'seems', 'appears', 'suggests', 'may',\n",
    "    'perhaps', 'probably', 'likely', 'presumably', 'assume', 'assumes', 'indicates',\n",
    "    'apparently', 'arguably', 'tends', 'potentially', 'often', 'sometimes', 'can', \n",
    "    'seem', 'think', 'estimated', 'considered', 'speculate', 'roughly'\n",
    "]\n",
    "certainty_words = [\n",
    "    'definitely', 'certainly', 'absolutely', 'clearly', 'undoubtedly', 'surely',\n",
    "    'yes', 'no', 'nothing', 'never', 'is', 'are', 'will', 'must', 'cannot',\n",
    "    'always', 'every', 'all', 'without a doubt', 'it is clear', 'proven', 'true', 'fact', 'obviously'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e6a8659-6321-4c27-83ef-052dcc2166c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text, word_list):\n",
    "    text = str(text).lower() # made lowercase to avoid case mismatches\n",
    "    return sum(text.count(word.lower()) for word in word_list)\n",
    "    \n",
    "df1['Hedging_words_count'] = df1['best_answer'].apply(lambda x: count_words(x, hedging_words))\n",
    "df1['Certainty_markers_count'] = df1['best_answer'].apply(lambda x: count_words(x, certainty_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a037840-8cf7-40c3-85e5-0dc9d402e20d",
   "metadata": {},
   "source": [
    "EXPLORING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9631c-5d4e-4859-b9c0-7e620b0f489a",
   "metadata": {},
   "source": [
    "We will be creating a new binary variable called 'ai_correct' to indicate the correctness of the answer LLM gives us (1 if the AI answer matches a correct answer and 0 if it doesnâ€™t). This variable captures exactly what we are interested in: whether the AI provides correct or trustful answers and specifically whether linguistic and structural features relate to factual accuracy. Since correctness is one of the main aspects we are analyzing, this binary measure is precise and straightforward. We expect that this variable will clearly show patterns in AI performance, such as differences across question types, categories, or response characteristics. We would expect that AI-generated answers with longer length and more certain linguistic cues to be more trustful and correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd95af-23cc-409d-9df2-0bb022ddf113",
   "metadata": {},
   "source": [
    "Our key explanatory variables are: \n",
    "a). Hedging_words_count, which measures the frequency of uncertainty phrases, such as 'might' 'possibly' and 'perhaps'.\n",
    "b). Certainty_markers_count, which measures the frequency of high-certainty phrases, such as 'definitely', 'clearly', and 'of course'.\n",
    "c). answer_length, which measures the word count of the LLM-generated answer.\n",
    "d). Category, which captures the domain of each question, such as Science, Politics, Economics, and Education and indicates contextual differences in model performance, revealing whether certain knowledge areas are more prone to factual errors or overconfident falsehoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e26ffc5c-c33c-400b-8777-ca3bcf974f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table of summary statistics of our key variables\n",
    "df1[['ai_correct', 'Hedging_words_count', 'Certainty_markers_count', 'answer_length']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8241f5e-3979-4b48-b52b-4dc002338c87",
   "metadata": {},
   "source": [
    "DATA VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f2d24-8be3-442a-882e-69ff5d37ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outcome Variable Plots\n",
    "counts = df1['ai_correct'].value_counts()\n",
    "counts = counts.reindex([True, False], fill_value=0)  # add False with 0 if missing\n",
    "percentages = counts / counts.sum() * 100\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(['True', 'False'], percentages.values, color=['skyblue', 'salmon'], label=['Correct', 'Incorrect'])\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.xlabel('AI Correct?')\n",
    "plt.title('Percentage of Correct vs Incorrect AI Responses')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.15))\n",
    "plt.show()\n",
    "\n",
    "#Key Explanatory Variable Plots\n",
    "type_counts = df1['type'].value_counts()\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(type_counts.index, type_counts.values, color=['lightgreen','salmon'], label=['Non-Adversarial', 'Adversarial'])\n",
    "plt.xlabel('Question Type')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.title('Number of Adversarial vs Non-Adversarial Questions')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1.15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4e866-567e-40c4-a224-2fc28d81fcf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
