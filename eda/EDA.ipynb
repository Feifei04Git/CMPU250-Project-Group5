{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990bee02-0738-43fc-b735-fe32ced18f5e",
   "metadata": {},
   "source": [
    "DATA LOADING and CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37a849a2-3824-4069-afa6-8a4784792945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to Install all required packages using conda (safer on Mac & Anaconda)\n",
    "#!conda install -y -c conda-forge pyarrow fsspec huggingface_hub datasets\n",
    "#!conda install -y -c conda-forge pyarrow pandas --force-reinstall\n",
    "#!pip install numpy pandas matplotlib scikit-learn statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a64abff7-f957-4a6f-86d9-bcd00a7d94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import statsmodels.formula.api as smf\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "66558400-c87b-4da2-b828-86c4f79c07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"truthfulqa/truthful_qa\", \"generation\")\n",
    "df1 = ds[\"validation\"].to_pandas()\n",
    "print(\"[generation]\", df1.shape, df1.columns.tolist())\n",
    "\n",
    "from huggingface_hub import HfFileSystem\n",
    "fs = HfFileSystem()\n",
    "with fs.open(\"datasets/truthfulqa/truthful_qa/multiple_choice/validation-00000-of-00001.parquet\", \"rb\") as f:\n",
    "    df2 = pd.read_parquet(f)\n",
    "print(\"[multiple_choice]\", df2.shape, df2.columns.tolist())\n",
    "\n",
    "df1.to_csv(\"truthful_qa_generation.csv\", index=False)\n",
    "df2.to_csv(\"truthful_qa_multiple_choice.csv\", index=False)\n",
    "\n",
    "df1[\"ai_correct\"] = df1.apply(lambda row: row[\"best_answer\"] in row[\"correct_answers\"], axis=1)\n",
    "print(\"[ok] df1 ready:\", df1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90398103-f229-4c63-b494-083f2c654be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"truthfulqa/truthful_qa\", \"generation\")\n",
    "\n",
    "hedging_words = [\n",
    "    'probable','possible','likely','probably','possibly','perhaps','maybe','may','might','could',\n",
    "    'apparently','seemingly','sometimes','suggest','appear','seem','often','usually',\n",
    "    'primarily','generally','largely','tend','tendency'\n",
    "]\n",
    "\n",
    "certainty_words = [\n",
    "    'certainly','undoubtedly','obviously','definitely','surely','absolutely',\n",
    "    'clearly','sure','in fact','guaranteed','proven','conclusively','evidently',\n",
    "    'beyond doubt','no doubt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd4ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure list columns\n",
    "def to_list_if_needed(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return list(x)\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return []\n",
    "    s = str(x).strip()\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        parts = re.findall(r\"'(.*?)'|\\\"(.*?)\\\"\", s)\n",
    "        parts = [p[0] or p[1] for p in parts]\n",
    "        return [p for p in parts if p]\n",
    "\n",
    "if not isinstance(df1.iloc[0]['correct_answers'], list):\n",
    "    df1['correct_answers'] = df1['correct_answers'].apply(to_list_if_needed)\n",
    "if not isinstance(df1.iloc[0]['incorrect_answers'], list):\n",
    "    df1['incorrect_answers'] = df1['incorrect_answers'].apply(to_list_if_needed)\n",
    "\n",
    "# remove incomplete observations\n",
    "before = len(df1)\n",
    "df1 = df1.dropna(subset=['best_answer'])\n",
    "df1 = df1[df1['correct_answers'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "df1 = df1[df1['incorrect_answers'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "after = len(df1)\n",
    "print(f\"[clean] dropped {before - after} rows\")\n",
    "\n",
    "# recompute ai_correct after cleaning\n",
    "df1['ai_correct'] = df1.apply(lambda r: r['best_answer'] in r['correct_answers'], axis=1)\n",
    "df1.to_csv(\"truthful_qa_generation_clean_question_level.csv\", index=False)\n",
    "print(\"[save] wrote truthful_qa_generation_clean_question_level.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ae104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new column\n",
    "df1['n_correct'] = df1['correct_answers'].apply(len)\n",
    "df1['n_incorrect'] = df1['incorrect_answers'].apply(len)\n",
    "df1['ai_correct'] = df1.apply(lambda r: 1 if r['best_answer'] in r['correct_answers'] else 0, axis=1)\n",
    "df1['answer_length_best'] = df1['best_answer'].astype(str).str.split().apply(len)\n",
    "df1.to_csv(\"truthful_qa_generation_prepared.csv\", index=False)\n",
    "print(\"[save] wrote truthful_qa_generation_prepared.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c86d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering\n",
    "hedge_patterns = [r\"\\b\" + re.escape(w) + r\"\\b\" for w in hedging_words]\n",
    "certainty_patterns = [r\"\\b\" + re.escape(w) + r\"\\b\" for w in certainty_words]\n",
    "\n",
    "def count_matches(text, patterns):\n",
    "    text = str(text).lower()\n",
    "    return sum(len(re.findall(p, text)) for p in patterns)\n",
    "\n",
    "df1['hedging_words_count'] = df1['best_answer'].apply(lambda x: count_matches(x, hedge_patterns))\n",
    "df1['certainty_markers_count'] = df1['best_answer'].apply(lambda x: count_matches(x, certainty_patterns))\n",
    "\n",
    "df1.to_csv(\"truthful_qa_generation_features.csv\", index=False)\n",
    "print(\"[save] wrote truthful_qa_generation_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restructuring to answer-level\n",
    "keep_cols = ['type', 'category', 'question', 'source']\n",
    "rows = []\n",
    "for _, r in df1.iterrows():\n",
    "    for a in r['correct_answers']:\n",
    "        rows.append({**{k: r.get(k, None) for k in keep_cols}, 'answer': a, 'correctness': 1})\n",
    "    for a in r['incorrect_answers']:\n",
    "        rows.append({**{k: r.get(k, None) for k in keep_cols}, 'answer': a, 'correctness': 0})\n",
    "\n",
    "answers_df = pd.DataFrame(rows)\n",
    "answers_df['answer_length']   = answers_df['answer'].astype(str).str.split().apply(len)\n",
    "answers_df['hedge_count']     = answers_df['answer'].apply(lambda x: count_matches(x, hedge_patterns))\n",
    "answers_df['certainty_count'] = answers_df['answer'].apply(lambda x: count_matches(x, certainty_patterns))\n",
    "answers_df.to_csv(\"truthful_qa_generation_answer_level.csv\", index=False)\n",
    "print(\"[save] wrote truthful_qa_generation_answer_level.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d446e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclusion criteria & final variables\n",
    "# question-level\n",
    "q = df1.dropna(subset=['best_answer'])\n",
    "q = q[q['correct_answers'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "q = q[q['incorrect_answers'].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "if 'n_correct' not in q.columns:  q['n_correct'] = q['correct_answers'].apply(len)\n",
    "if 'n_incorrect' not in q.columns: q['n_incorrect'] = q['incorrect_answers'].apply(len)\n",
    "if 'answer_length_best' not in q.columns: q['answer_length_best'] = q['best_answer'].astype(str).str.split().apply(len)\n",
    "if 'hedging_words_count' not in q.columns: q['hedging_words_count'] = q['best_answer'].apply(lambda x: count_matches(x, hedge_patterns))\n",
    "if 'certainty_markers_count' not in q.columns: q['certainty_markers_count'] = q['best_answer'].apply(lambda x: count_matches(x, certainty_patterns))\n",
    "if 'ai_correct' not in q.columns: q['ai_correct'] = q.apply(lambda r: 1 if r['best_answer'] in r['correct_answers'] else 0, axis=1)\n",
    "\n",
    "q = q[['type','category','question','best_answer','source',\n",
    "       'n_correct','n_incorrect','answer_length_best',\n",
    "       'hedging_words_count','certainty_markers_count','ai_correct']]\n",
    "q.to_csv(\"truthful_qa_generation_final_question_level.csv\", index=False)\n",
    "print(\"[save] wrote truthful_qa_generation_final_question_level.csv\")\n",
    "\n",
    "# answer-level\n",
    "a = answers_df.dropna(subset=['answer'])\n",
    "a = a[a['answer'].astype(str).str.strip().ne('')]\n",
    "\n",
    "if 'answer_length' not in a.columns:   a['answer_length'] = a['answer'].astype(str).str.split().apply(len)\n",
    "if 'hedge_count' not in a.columns:     a['hedge_count'] = a['answer'].apply(lambda x: count_matches(x, hedge_patterns))\n",
    "if 'certainty_count' not in a.columns: a['certainty_count'] = a['answer'].apply(lambda x: count_matches(x, certainty_patterns))\n",
    "\n",
    "a = a[['type','category','question','source','answer',\n",
    "       'correctness','answer_length','hedge_count','certainty_count']]\n",
    "a.to_csv(\"truthful_qa_generation_final_answer_level.csv\", index=False)\n",
    "print(\"[save] wrote truthful_qa_generation_final_answer_level.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e6a8659-6321-4c27-83ef-052dcc2166c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text, word_list):\n",
    "    text = str(text).lower() # made lowercase to avoid case mismatches\n",
    "    return sum(text.count(word.lower()) for word in word_list)\n",
    "    \n",
    "df1['Hedging_words_count'] = df1['best_answer'].apply(lambda x: count_words(x, hedging_words))\n",
    "df1['Certainty_markers_count'] = df1['best_answer'].apply(lambda x: count_words(x, certainty_words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a037840-8cf7-40c3-85e5-0dc9d402e20d",
   "metadata": {},
   "source": [
    "EXPLORING THE DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b9631c-5d4e-4859-b9c0-7e620b0f489a",
   "metadata": {},
   "source": [
    "We will be creating a new binary variable called 'ai_correct' to indicate the correctness of the answer LLM gives us (1 if the AI answer matches a correct answer and 0 if it doesnâ€™t). This variable captures exactly what we are interested in: whether the AI provides correct or trustful answers and specifically whether linguistic and structural features relate to factual accuracy. Since correctness is one of the main aspects we are analyzing, this binary measure is precise and straightforward. We expect that this variable will clearly show patterns in AI performance, such as differences across question types, categories, or response characteristics. We would expect that AI-generated answers with longer length and more certain linguistic cues to be more trustful and correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd95af-23cc-409d-9df2-0bb022ddf113",
   "metadata": {},
   "source": [
    "Our key explanatory variables are: \n",
    "a). Hedging_words_count, which measures the frequency of uncertainty phrases, such as 'might' 'possibly' and 'perhaps'.\n",
    "b). Certainty_markers_count, which measures the frequency of high-certainty phrases, such as 'definitely', 'clearly', and 'of course'.\n",
    "c). answer_length, which measures the word count of the LLM-generated answer.\n",
    "d). Category, which captures the domain of each question, such as Science, Politics, Economics, and Education and indicates contextual differences in model performance, revealing whether certain knowledge areas are more prone to factual errors or overconfident falsehoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e26ffc5c-c33c-400b-8777-ca3bcf974f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table of summary statistics of our key variables\n",
    "df1[['ai_correct', 'Hedging_words_count', 'Certainty_markers_count', 'answer_length']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8241f5e-3979-4b48-b52b-4dc002338c87",
   "metadata": {},
   "source": [
    "DATA VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318f2d24-8be3-442a-882e-69ff5d37ffe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outcome Variable Plots\n",
    "counts = df1['ai_correct'].value_counts()\n",
    "counts = counts.reindex([True, False], fill_value=0)  # add False with 0 if missing\n",
    "percentages = counts / counts.sum() * 100\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(['True', 'False'], percentages.values, color=['skyblue', 'salmon'], label=['Correct', 'Incorrect'])\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.xlabel('AI Correct?')\n",
    "plt.title('Percentage of Correct vs Incorrect AI Responses')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.15))\n",
    "plt.show()\n",
    "\n",
    "#Key Explanatory Variable Plots\n",
    "type_counts = df1['type'].value_counts()\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(type_counts.index, type_counts.values, color=['lightgreen','salmon'], label=['Non-Adversarial', 'Adversarial'])\n",
    "plt.xlabel('Question Type')\n",
    "plt.ylabel('Number of Questions')\n",
    "plt.title('Number of Adversarial vs Non-Adversarial Questions')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1.15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4e866-567e-40c4-a224-2fc28d81fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer length vs correctness\n",
    "plt.figure(figsize=(6,4))\n",
    "df1.boxplot(column='answer_length_best', by='ai_correct', patch_artist=True,\n",
    "            boxprops=dict(facecolor='lightblue'), medianprops=dict(color='red'))\n",
    "plt.title('Answer Length by AI Correctness')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('AI Correct (1 = Correct, 0 = Incorrect)')\n",
    "plt.ylabel('Answer Length (words)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linguistic features by category\n",
    "agg = df1.groupby('category')[['hedging_words_count','certainty_markers_count']].mean().sort_values('hedging_words_count', ascending=False)\n",
    "agg.plot(kind='bar', figsize=(8,5))\n",
    "plt.title('Average Hedging and Certainty Counts by Question Category')\n",
    "plt.xlabel('Question Category')\n",
    "plt.ylabel('Average Count')\n",
    "plt.legend(['Hedging Words','Certainty Markers'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
